# core-ML-from-scratch

This repository contains core machine learning algorithms implemented from scratch using only NumPy. The goal is to understand how these algorithms work internally without relying on high-level libraries like scikit-learn or TensorFlow.

Files:

- perceptron_back_prop.ipynb  
  Implements a single-layer neural network with backpropagation for binary classification on synthetic 2D data.

- adaboost_implementation.ipynb  
  AdaBoost with decision stumps on MNIST (digits 0 vs 1), using PCA for preprocessing and exponential loss with early stopping.

- gradient_boosting.ipynb  
  Gradient boosting for regression using decision stumps and both squared and absolute loss on synthetic sin-cos data.

Algorithms Covered:

- Perceptron and Neural Network Backpropagation  
- AdaBoost (Ensemble Method)  
- Gradient Boosting (Regression)  
- Manual PCA  
- Gradient-based optimization and loss analysis

Instructions:

Each notebook runs independently. Use any Python environment with NumPy and Matplotlib. Jupyter Notebook is recommended for step-by-step execution and plots.

---
